{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d871bf",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "Your task is to program an agent to find the optimal policy for navigating a labyrinth from a specified starting point to a goal point using the Value Iteration algorithm.\n",
    "\n",
    "**Step 1: Familiarize with the Environment**\n",
    "- Understand the structure of the `Labyrinth` class and how it represents the labyrinth environment, including walls, the starting point, and the goal.\n",
    "- Familiarize yourself with how the `Agent` class is structured, and how it interacts with the labyrinth environment.\n",
    "\n",
    "**Step 2: Implement Value Iteration**\n",
    "- Create a function or method to implement the Value Iteration algorithm.\n",
    "- You'll need to initialize a utility table with zeros and iteratively update the utilities of each state (i.e., each cell in the labyrinth) based on the Bellman equation.\n",
    "- The stopping criterion for Value Iteration is when the maximum change in utility is less than a small threshold, say 0.01.\n",
    "- Once the utilities have converged, use them to compute the optimal policy, which specifies the best action to take in each state.\n",
    "\n",
    "**Step 3: Modify the Agent Class**\n",
    "- Modify the `act` method of the `Agent` class to use the optimal policy derived from Value Iteration instead of taking random actions.\n",
    "- Optionally, you can also modify the `update` method to incorporate any additional learning or updating you wish to implement.\n",
    "\n",
    "**Step 4: Run the Simulation**\n",
    "- Run the provided simulation loop, where the agent is placed in the labyrinth and must navigate to the goal.\n",
    "- Observe how the agent's behavior changes as it learns the optimal policy.\n",
    "- You might want to add some print statements or other logging to help visualize the agent's path through the labyrinth and how it improves over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee24ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Labyrinth:\n",
    "    def __init__(self, rows, cols, walls, start, goal):\n",
    "        self.grid = np.zeros((rows, cols))\n",
    "        for wall in walls:\n",
    "            self.grid[wall] = -1  # Assign -1 for walls\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.current_position = start\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.start\n",
    "        return self.current_position\n",
    "\n",
    "    def step(self, action):\n",
    "        # Assume actions are encoded as (delta_row, delta_col)\n",
    "        new_position = (\n",
    "            self.current_position[0] + action[0],\n",
    "            self.current_position[1] + action[1],\n",
    "        )\n",
    "        if self.is_valid_move(new_position):\n",
    "            self.current_position = new_position\n",
    "        reward = 1 if self.current_position == self.goal else 0\n",
    "        return self.current_position, reward\n",
    "\n",
    "    def is_valid_move(self, position):\n",
    "        rows, cols = self.grid.shape\n",
    "        # return (0 <= position[0] < rows\n",
    "        #         and 0 <= position[1] < cols\n",
    "        #         and self.grid[position] != -1)\n",
    "        if not 0 <= position[0] < rows:\n",
    "            return False\n",
    "        if not 0 <= position[1] < cols:\n",
    "            return False\n",
    "        if self.grid[position] == -1:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def done(self):\n",
    "        return self.current_position == self.goal\n",
    "\n",
    "    def display(self, turn, move):\n",
    "        grid_copy = self.grid.copy()\n",
    "        grid_copy[self.current_position] = 2\n",
    "        grid_copy[self.goal] = 9\n",
    "        print(f'{t=} {move=}')\n",
    "        print(grid_copy)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c864eb-6276-4eca-8053-4f3c242103f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.8999999999999996,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 3): 2.499999999999999,\n",
       " (2, 0): 1.4999999999999993,\n",
       " (2, 2): 2.499999999999999,\n",
       " (2, 3): 0.0,\n",
       " (3, 0): 0.0,\n",
       " (3, 1): 2.499999999999999,\n",
       " (3, 2): 0.0,\n",
       " (3, 3): 2.499999999999999}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        environment: Labyrinth,\n",
    "        *,\n",
    "        gamma: float = 0.6,\n",
    "    ):\n",
    "        self.environment = environment\n",
    "        rows, cols = labyrinth.grid.shape\n",
    "        self.states = [\n",
    "            (i, j) for i in range(rows) for j in range(cols)\n",
    "            if labyrinth.grid[i, j] != -1\n",
    "        ]\n",
    "        self.values = dict.fromkeys(self.states, 0)\n",
    "        self.gamma = gamma  # discount\n",
    "        self.actions = [\n",
    "            (0, 1),\n",
    "            (0, -1),\n",
    "            (1, 0),\n",
    "            (-1, 0),\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "        # self.values = dict.fromkeys(self.states, 0)\n",
    "        ...\n",
    "\n",
    "    def act(self, state) -> tuple[int, int]:\n",
    "        if random.random() < 0.2:\n",
    "            return random.choice(self.actions)\n",
    "        row, col = state\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        for action in self.actions:\n",
    "            new_coord = row + action[0], col + action[1]\n",
    "            if self.environment.is_valid_move(new_coord):\n",
    "                value = self.values[new_coord]\n",
    "                if value > best_value:\n",
    "                    best_action, best_value = action, value\n",
    "        if best_action is None:  # kinda unnecessary, but mypy complains\n",
    "            best_action = random.choice(self.actions)\n",
    "        return best_action\n",
    "\n",
    "    def update(self, action, state, reward) -> None:\n",
    "        # Labyrinth step method gives just curr state and it's reward\n",
    "        # we need future states & rewards for value\n",
    "        for state in self.states:\n",
    "            row, col = state\n",
    "            new_value = float('-inf')\n",
    "\n",
    "            for action in self.actions:\n",
    "                new_state = row + action[0], col + action[1]\n",
    "                if self.environment.is_valid_move(new_state):\n",
    "                    reward = 1 if new_state == self.environment.goal else 0\n",
    "                    value = reward + self.gamma * self.values[new_state]\n",
    "                    new_value = max(new_value, value)\n",
    "                    self.values[new_state] = new_value\n",
    "\n",
    "    # def update(self, action, state, reward) -> None:\n",
    "    #     new_state = state[0] + action[0], state[1] + action[1]\n",
    "    #     if self.environment.is_valid_move(new_state):\n",
    "    #         self.values[state] = reward + self.gamma * self.values[new_state]\n",
    "\n",
    "\n",
    "# Define labyrinth\n",
    "labyrinth = Labyrinth(4, 4, {(1, 1), (2, 1), (1, 2)}, (0, 0), (3, 3))\n",
    "agent = Agent(environment=labyrinth)\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "T = 100\n",
    "\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state = labyrinth.reset()\n",
    "    agent.reset()\n",
    "    for t in range(T):\n",
    "        action = agent.act(state)\n",
    "        state, reward = labyrinth.step(action)\n",
    "        agent.update(action, state, reward)\n",
    "        labyrinth.display(t, action)\n",
    "        if labyrinth.done():\n",
    "            print(f'{episode=}. success')\n",
    "            break\n",
    "\n",
    "# agent.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6ad2e-8c5d-42f7-932e-8083b111b4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
